	As my base motivation for this piece, I wanted to create a piece based on “continuous time” motion - specifically, I wanted to avoid discrete rhythms as much as possible. 

	I started with a “wind synth”, which was a Max patch consisting of a pink noise source feeding through a stack of resonant filters all tuned to the same frequency. I wanted to be able to pitch the noise, and by varying the Q value I could have the sound be very windy (Q < 4) or clearly pitched (Q > 20). The synth had 3 independent voices, with each having independent frequency and Q parameters. The technique I started with for my draft was to use complicated LFOs (created via mix of FM and additive synthesis) to vary the pitch and q values, with the hope that by creating slow complex waves, that in and of itself would be interesting enough to create a structure for the piece. In practice I found that while this technique created interesting “phrases”, it did not lend itself well to higher level structure. 

	I made several additions to add more narrative motion to the piece. Firstly, I modified the pitch waves such that instead of being continuous, they would snap to the nearest note of some predefined chord. I was happy to find that this still produced a generally legato sound.  Next, I populated 7 random chords( i.e., a random selection of 3 pitch chroma) in a full binary tree structure (depth 3), and a random walk through the tree determined what chord tones the pitch waves would snap to. Secondly, I populated another full binary tree structure (depth 3),  with node holding a pair of waves (one an LFO controlling frequencies, and another a wave controlling gQ values). Each voice had its own pointer in this tree, and a random walk for each voice-pointer determined what waves were playing for each voice. The random walk steps for the chords happened at a random rhythm (generated at the start of the piece) that was 20 seconds long. The random walk steps for each of the wave-voice-pointers happened every 20 seconds. 

	Overall, this did improve the piece - the quantization of the frequencies to chord tones made the overall sonic texture more understandable, and the random walk elements helped give it a sense of “movement”, however, it still lacks a sense of “direction” or higher level structure. It’s possible that the 3 separate random walks between the two voices created too much variation for a listener to track at once, preventing listeners from being able to identify the different wave patterns. Coordinating between the three voices in some way (potentially by assigning one “lead” voice and having the LFO waves of the other two voices be harmonics of the lead), could have helped reveal the changes more clearly. When running the piece as a single voice position, the wave changes were more discernible, but the piece seemed quite “empty”. 

	This software was built using Python, SuperCollider, and MaxMSP, and used the pyOSC library https://github.com/ptone/pyosc. The file windgen_final.ipynb is the “score” for this piece, with all other code (besides OSC.py) being personal libraries I’ve built. 